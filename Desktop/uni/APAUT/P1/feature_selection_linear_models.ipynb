{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Supervised Feature Selection with Regularized Linear Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection methods overview\n",
    "\n",
    "This item is based con the first paper.\n",
    "\n",
    "**Goals of feature selection**\n",
    "\n",
    "Scenarios related to few tens of samples but thousands dimensions: microarray data, \n",
    "\n",
    "1. To avoid overfiting and improve model performance, prediction performance in the case of supervised classification and better cluster detection in unsupervised scenarios.\n",
    "\n",
    "2. To provide more efficient models\n",
    "\n",
    "3. To gain a deeper insight into the underlying processes that generated the data. The excess of dimensionality difficult the understanding.\n",
    "\n",
    "The problem is related to find the optimal model parameters for the optimal feature subset. So, the model parameters becomes dependent of the features selected and need to be computed more or less coupled with the guessing of model parameters.\n",
    "\n",
    "From less (zero) to more coupled computation, we have three strategies:\n",
    "\n",
    "1. Filter techniques. Two step process, first the filtering, then the training of the model. Take into account only the properties of the data and in some cases a certain amount of prior knowledge. Therefore it's independent of the classification method. In its most simplest form ignores dependences on the data (univariate).\n",
    "\n",
    "    Examples: Euclidean distance, i-test Information gain, Markov blanket filter\n",
    "\n",
    "2. Wrapper methods. Once selected a candidate subset of features, the classification model is evaluated by training and testing the model. This is iterated over a ensemble of candidate subsets, and the model (with his feature subsets) selected is the model with the best accuracy. \n",
    "    \n",
    "    It's very important to construct a good searching algorithm of subsets, in order to reduce the number of sets to model with. This methods are dependent of the classifier, model feature dependencies and have the risk to be bind to a local optima. With randomizing techniques this problem is bypassed to some extent. \n",
    "    \n",
    "    Examples: Sequential forward selection (SFS) , Sequential backward elimination, Simulated annealing, Randomized hill climbing, Genetic algorithms.\n",
    "\n",
    "3. Embedded methods. The search of the optimal subset of features is built into the classifier. Have the advantage that they include the interaction with the classification model, while at the same time being far less computationally intensive than wrapper methods.\n",
    "\n",
    "    Examples: Decision trees Weighted naive Bayes, Feature selection using the weight vector of SVM, AROM\n",
    "    \n",
    "### AROM methods\n",
    "\n",
    "The acronym derives from *Approximation of Minimization zeRO-norm*\n",
    "\n",
    "The problem is obtain a linear predictor $h$, minimizing the number of independent variables (features) without loss of accuracy:\n",
    "\n",
    "$$h(\\mathbf{x}) = sign(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "for $n$ samples $x_i \\in \\mathbb{R}^n$ and $m$ labels $y_i \\in \\{\\pm1\\}$.\n",
    "\n",
    "The accuracy constraint requires correspondence of sign \n",
    "\n",
    "$sign(y_i) \\cdot sign(h_i) > 0$ or in other form $y_i \\cdot h_i = 1$\n",
    "\n",
    "or less restrictive, enabling $\\mathbf{w}$ to scale freely $y_i \\cdot h_i \\ge 1$\n",
    "\n",
    "so \n",
    "\n",
    "$$y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) \\ge 1$$\n",
    "\n",
    "The minimization is done with a norm defined over the vectorial space of $\\mathbf{w}$. One approach is to minimize the zero-norm, that is, the number of components of the vector (number of non null $w_i$). But it's know to be a NP-Hard problem.\n",
    "\n",
    "It's more adequate compute over a 1-norm or a 2-norm. In the second paper, the author deduce a suitable form for the function that could be minimized, taken into account the former constraint:\n",
    "\n",
    "$$\\displaystyle\\sum_{j=1}^n ln(|w_j| + \\epsilon)$$\n",
    "\n",
    "The term $\\epsilon$ is included to protect from zero values inside logarithm.\n",
    "\n",
    "AROM methods are therefore feature selection embedded methods.\n",
    "\n",
    "**l1-AROM** and **l2-AROM** (in this case by means of a 2-norm minimization) algorithms optimize this algorithm by iterative rescaling of inputs and doing a smooth feature selection since the weight coefficients along some dimensions progressively drop below the machine precision while other dimensions become more significant.\n",
    "\n",
    "### AROM semi-supervised\n",
    "\n",
    "Third and Fourth papers explore a improvement of these previous described methods.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Classification of microarray data: few tens of samples against several thousand dimensions (genes).\n",
    "\n",
    "**Key differential strategy**\n",
    "\n",
    "Extend AROM methods by means of partial supervision on the dimensions of a feature selection procedure. The technique proposes to use of prior knowledge to guide feature selection, but flexible enough to let the final selection depart from it if necessary to optimize the classification objective.\n",
    "\n",
    "The preferential features are previously selected from similar datasets in large microarray databases because it's known that different sub-samples of patients lead to very similar sets of biomarkers, as expected if we are aware that the biological process explaining the outcome is common among different patients.\n",
    "\n",
    "This datasets are called source datasets and we expect that the prediction for a similar feature vector is the same than the prediction for this vector in our dataset (the target).\n",
    "\n",
    "*In third paper prior knowledge is incorporated by biological information*\n",
    "\n",
    "So, if we have some knowledge on the relative importance of each feature (either from actual prior knowledge or from a related dataset), the supervised AROM objective can be modified by adding a prior relevance vector $\\beta = [\\beta_1,...,\\beta_n]$  defined over the $n$ dimensions and where $\\beta_j >0$ is the prior relevance of the $j$ feature.\n",
    "\n",
    "So in this case, the function to minimize in the case of 1-norm is:\n",
    "\n",
    "$$\\displaystyle\\sum_{j=1}^n \\frac{1}{\\beta_j} ln(|w_j| + \\epsilon)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2-AROM\n",
    "Describe how the provided implementation of L2-AROM works. See [2, 3, 4] for specific details. Next, implement a variable ranking approach based on the PS-L2-AROM method, as described in [4], using the provided implementation of L2-AROM.\n",
    "\n",
    "You should introduce the possibility in the previous implementation to specify the initial value of of the scaling vector z. By default this vector should be equal to a vector with all components equal to one. By increasing or reducing these values, one should be able to favor, or make more difficult the selection of specific features. This will lead to the method PS-L2-AROM, in which some sort of prior-knowledge about the importance of each feature can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**\n",
    "\n",
    "Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a {\\displaystyle p} p-dimensional vector (a list of {\\displaystyle p} p numbers), and we want to know whether we can separate such points with a {\\displaystyle (p-1)} (p-1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So **we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized**. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier.\n",
    "\n",
    "But often the target are not linearly separable in that space. For this reason,  the original finite-dimensional space be mapped into a much higher-dimensional space, making the separation viable in the new space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function {\\displaystyle k(x,y)} {\\displaystyle k(x,y)} selected to suit the problem.[5] \n",
    "\n",
    "The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vector is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters {\\displaystyle \\alpha _{i}} \\alpha _{i} of images of feature vectors {\\displaystyle x_{i}} x_{i} that occur in the data base.[clarification needed] With this choice of a hyperplane, the points {\\displaystyle x} x in the feature space that are mapped into the hyperplane are defined by the relation {\\displaystyle \\textstyle \\sum _{i}\\alpha _{i}k(x_{i},x)={\\text{constant}}.} {\\displaystyle \\textstyle \\sum _{i}\\alpha _{i}k(x_{i},x)={\\text{constant}}.} Note that if {\\displaystyle k(x,y)} {\\displaystyle k(x,y)} becomes small as {\\displaystyle y} y grows further away from {\\displaystyle x} x, each term in the sum measures the degree of closeness of the test point {\\displaystyle x} x to the corresponding data base point {\\displaystyle x_{i}} x_{i}. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points {\\displaystyle x} x mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.\n",
    "\n",
    "**RFE**\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "RFECV performs RFE in a cross-validation loop to find the optimal number of features.\n",
    "\n",
    "\n",
    "**L2-AROM**\n",
    "\n",
    "In the present work we rely on another embedded selection method with linear models, called l1-AROM [25]. This specific choice is motivated by the possibil- ity to extend this approach in a simple yet efficient way to perform transfer learning by biasing the optimization procedure towards certain dimensions. We proposed recently such a partially supervised (PS) extension [26] but the favored dimensions were then defined from prior knowledge. In the context of microarray data, molecular biologists may indeed sometimes guess that a few genes should be considered a priori more relevant. In the present work, we do not use such prior knowledge but rather related datasets, hence performing inductive trans- fer learning at the feature level. The additional benefits are a fully automated feature selection procedure and the possibility to choose the number of features to be transferred independently of some expert knowledge. A practical approx- imation of this technique reduces to learn linear SVMs with iterative rescaling of the inputs. The rescaling factors depend here on previously selected features from existing datasets.\n",
    "\n",
    "At step k = 0, initialize wk = β Iterate until convergence:\n",
    "1 minw ||w||2\n",
    "subject to: yi (w · (xi ∗ wk ) + b) ≥ 1\n",
    "2 Let (w ̄ ) be the solution, set wk+1 ← wk ∗w ̄ ∗β\n",
    " \n",
    "**SVC**\n",
    "C-Support Vector Classification.\n",
    "The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
    "The multiclass support is handled according to a one-vs-one scheme.\n",
    "For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.\n",
    "\n",
    "\n",
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n",
    "\n",
    "\n",
    "\n",
    "The l2-AROM method further approximates this optimization by replacing the l1-norm by the l2-norm. Even though such an approximation may result in a less sparse solution, it is very efficient in practice when m ≪ n. Indeed, a dual formulation may be used and the final algorithm boils down to a linear SVM estimation with iterative rescaling of the inputs. \n",
    "\n",
    "**A standard SVM solver can be iteratively called on properly rescaled inputs. A smooth feature selection occurs during this iterative process since the weight coefficients along some dimensions progressively drop below the machine precision while other dimensions become more significant. A final ranking on the absolute values of each dimension can be used to obtain a fixed number of features.**\n",
    "\n",
    "\n",
    "**T-test**\n",
    "\n",
    "Assuming it to be a binary classification problem, where each sample can be classified either into class C1 or class C2, t-Statistics helps us to evaluate that whether the values of a particular feature for class C1 is significantly different from values of same feature for class C2. If this holds, then the feature can helps us to better differentiate our data.\n",
    "\n",
    "e.g. Does the salary of a person impact his chances to get a loan ? Here we will calculate mean and variance of the following observations separately :\n",
    "\n",
    "Salaries of individuals when the loan was approved\n",
    "Salaries of individuals when the loan was not approved\n",
    "and then we will use t-statistics to check whether these two samples are significantly different or not.\n",
    "\n",
    "t- Statistics is computed using:\n",
    "\n",
    "\n",
    "where 𝑢𝑖𝑗 denotes mean of ith feature 𝑋𝑖 for class 𝐶𝑗and 𝑠𝑖𝑔𝑚𝑎𝑖𝑗denotes Standard Deviation of ith feature 𝑋𝑖 for class 𝐶𝑗 . The class index is denoted by j i.e. j =1 or j=2.\n",
    "\n",
    "After calculating the values of t-Statistic for each feature, we sort these values in descending order in order to select the important the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.feature_selection import RFE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##\n",
    "# X is numpy array witht the data (rows are data instances)\n",
    "# Y is a numpy vector with the class labels (-1 or 1)\n",
    "# C is the regularization coefficient of the SVM\n",
    "# threshold is the threshold value to drop features in L2AROM\n",
    "\n",
    "# 1. At step k = 0, initialize z = (1, ..., 1) \n",
    "# 2. Iterate until convergence:\n",
    "    # 1.1 minw ||w||2 Subject to: yi (w · (xi ∗ z ) + b) ≥ 1 \n",
    "    # 1.2 Let (w) be the solution, set w_new ← z ∗ w\n",
    "\n",
    "#\n",
    "#Relevance vector β\n",
    "#Prior relevance of feature j encoded in βj .\n",
    "#The more (a priori) relevant feature j, the higher βj. If no information on j, βj = 1.\n",
    "\n",
    "def variable_ranking(X, Y, C = 1, b=None, threshold=1e-10, feature_len=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy X to modify it later\n",
    "\n",
    "    final_X = X.copy()\n",
    "    print(\"First final_X\", final_X)\n",
    "    # Initialice w_k = (1,....,1)\n",
    "    # 0. At step k = 0, initialize z = (1, ..., 1) /b\n",
    "    z = b.copy()\n",
    "    print(\"Z\", z)\n",
    "    # Number of attributes\n",
    "\n",
    "    length = z.shape[0]\n",
    "    print(\"#Features\", length)\n",
    "\n",
    "    # Array that stores the elimination order, being the higher number the first attribute \n",
    "    # that is eliminated and 1 the last one\n",
    "\n",
    "    elimination_order = np.zeros(length, dtype = int)\n",
    "    print(\"#Elimination order\", elimination_order)\n",
    "    original_feature_indices = np.arange(0, length, dtype = int)\n",
    "    print(\"#Init feature indices\", original_feature_indices)\n",
    "    clf = SVC(kernel = \"linear\", C = C, random_state = 0)\n",
    "    print(clf)\n",
    "    iter_without_dropping = 0\n",
    "    n_removed_features = 0\n",
    "    \n",
    "    # 2. Iterate until convergence:\n",
    "    while iter_without_dropping < 20 and length > feature_len:\n",
    "\n",
    "        # Fit the SVC and compute z\n",
    "        print(\"ones\", np.ones(X.shape[ 0 ]))\n",
    "        # xi ∗ z\n",
    "        print(\"outer\", np.outer(np.ones(X.shape[ 0 ]), z)) \n",
    "        print(\"final_X\", final_X * np.outer(np.ones(X.shape[ 0 ]), z))\n",
    "        # 2.1 minw ||w||2 Subject to: yi (w · (xi ∗ z ) + b) ≥ 1 \n",
    "        clf.fit(final_X * np.outer(np.ones(X.shape[ 0 ]), z), Y)\n",
    "        # w = coef_\n",
    "        print(\"Coefs\", clf.coef_)\n",
    "        #2.2 Let (w=coef_) be the solution, set z_new ← z ∗ w * b\n",
    "        z *= np.abs(clf.coef_[0])*b # In absolute value\n",
    "#         print(\"------Z\", z)\n",
    "#         print(\"------B\", b)\n",
    "#         print(\"------ZB\", z*b)\n",
    "        #clf devuelve los coeficientes w y con ellos escalamos los w\n",
    "        #aprovechamos para calzarnos los z con un coeficiente pequeño\n",
    "        n_features_to_drop = np.sum(z < threshold)\n",
    "        \n",
    "        if n_features_to_drop == 0:\n",
    "            iter_without_dropping += 1\n",
    "        else:\n",
    "            iter_without_dropping = 0\n",
    "            print(\"@@Z to remove\", z, z[ z < threshold ])\n",
    "            remove_order = np.argsort(z[ z < threshold ])\n",
    "            print(\"@@@Threshold\", z < threshold)\n",
    "            print(\"@@@Remove order\", remove_order)\n",
    "            print(\"@@@Elimination order\", original_feature_indices[ z < threshold ])\n",
    "            print(\"@@@Elimination order 2\", original_feature_indices[ z < threshold ][ remove_order ])\n",
    "            elimination_order[ original_feature_indices[ z < threshold ][ remove_order ] ] = \\\n",
    "                np.arange(0, n_features_to_drop) + n_removed_features + 1\n",
    "            print(\"@@@Elimination order\", original_feature_indices[ z < threshold ])\n",
    "            print(\"@@@Elimination order 2\", original_feature_indices[ z < threshold ][ remove_order ])\n",
    "            print(\"@@@@elimination_order 3\", elimination_order)\n",
    "            n_removed_features += n_features_to_drop\n",
    "            length -= n_features_to_drop\n",
    "        \n",
    "            # Delete from X, z and original_features the selected attributes \n",
    "\n",
    "            final_X = final_X[ :, z >= threshold ]\n",
    "            original_feature_indices = original_feature_indices[ z >= threshold ]\n",
    "            b = b[ z >= threshold ]\n",
    "            z = z[ z >= threshold ]\n",
    "\n",
    "    # We remove all remaining features\n",
    "\n",
    "    if length > 0:\n",
    "            remove_order = np.argsort(z)\n",
    "            elimination_order[ original_feature_indices[ remove_order ] ] = \\\n",
    "                np.arange(0, length) + n_removed_features + 1\n",
    "    print(elimination_order)\n",
    "    print(np.argsort(elimination_order))\n",
    "    #ranking of features for more to less significance\n",
    "    return np.argsort(-elimination_order)  # So array starts at 0 (python indexing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X100_g_at</th>\n",
       "      <th>X1000_at</th>\n",
       "      <th>X1001_at</th>\n",
       "      <th>X1002_f_at</th>\n",
       "      <th>X1003_s_at</th>\n",
       "      <th>X1004_at</th>\n",
       "      <th>X1005_at</th>\n",
       "      <th>X1006_at</th>\n",
       "      <th>X1007_s_at</th>\n",
       "      <th>X1008_f_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX.ThrX.5_at</th>\n",
       "      <th>AFFX.ThrX.M_at</th>\n",
       "      <th>AFFX.TrpnX.3_at</th>\n",
       "      <th>AFFX.TrpnX.5_at</th>\n",
       "      <th>AFFX.TrpnX.M_at</th>\n",
       "      <th>AFFX.YEL002c.WBP1_at</th>\n",
       "      <th>AFFX.YEL018w._at</th>\n",
       "      <th>AFFX.YEL021w.URA3_at</th>\n",
       "      <th>AFFX.YEL024w.RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.234793</td>\n",
       "      <td>6.494211</td>\n",
       "      <td>4.853264</td>\n",
       "      <td>3.527822</td>\n",
       "      <td>5.575283</td>\n",
       "      <td>5.630715</td>\n",
       "      <td>7.070994</td>\n",
       "      <td>3.586507</td>\n",
       "      <td>8.607721</td>\n",
       "      <td>8.376150</td>\n",
       "      <td>...</td>\n",
       "      <td>4.124928</td>\n",
       "      <td>3.130851</td>\n",
       "      <td>2.983105</td>\n",
       "      <td>3.286748</td>\n",
       "      <td>3.632831</td>\n",
       "      <td>3.200749</td>\n",
       "      <td>3.157482</td>\n",
       "      <td>3.572550</td>\n",
       "      <td>3.201209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.967237</td>\n",
       "      <td>6.632175</td>\n",
       "      <td>4.320490</td>\n",
       "      <td>3.535030</td>\n",
       "      <td>5.505270</td>\n",
       "      <td>5.173343</td>\n",
       "      <td>7.826527</td>\n",
       "      <td>3.470474</td>\n",
       "      <td>6.871599</td>\n",
       "      <td>8.732676</td>\n",
       "      <td>...</td>\n",
       "      <td>4.089809</td>\n",
       "      <td>3.030838</td>\n",
       "      <td>2.710369</td>\n",
       "      <td>3.204168</td>\n",
       "      <td>3.721313</td>\n",
       "      <td>3.080551</td>\n",
       "      <td>2.908750</td>\n",
       "      <td>2.980353</td>\n",
       "      <td>3.264706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.026961</td>\n",
       "      <td>6.510959</td>\n",
       "      <td>4.267634</td>\n",
       "      <td>3.387379</td>\n",
       "      <td>5.906008</td>\n",
       "      <td>5.321219</td>\n",
       "      <td>7.857653</td>\n",
       "      <td>3.292397</td>\n",
       "      <td>7.521978</td>\n",
       "      <td>8.636165</td>\n",
       "      <td>...</td>\n",
       "      <td>3.693827</td>\n",
       "      <td>2.755653</td>\n",
       "      <td>2.526112</td>\n",
       "      <td>3.254250</td>\n",
       "      <td>3.362329</td>\n",
       "      <td>2.862432</td>\n",
       "      <td>3.048200</td>\n",
       "      <td>3.247433</td>\n",
       "      <td>3.061890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.123875</td>\n",
       "      <td>6.155900</td>\n",
       "      <td>4.114608</td>\n",
       "      <td>3.380995</td>\n",
       "      <td>5.891499</td>\n",
       "      <td>5.602339</td>\n",
       "      <td>8.285221</td>\n",
       "      <td>3.636381</td>\n",
       "      <td>8.148127</td>\n",
       "      <td>8.472201</td>\n",
       "      <td>...</td>\n",
       "      <td>4.345752</td>\n",
       "      <td>3.122182</td>\n",
       "      <td>2.656120</td>\n",
       "      <td>3.530544</td>\n",
       "      <td>3.515947</td>\n",
       "      <td>3.026449</td>\n",
       "      <td>3.231532</td>\n",
       "      <td>3.762868</td>\n",
       "      <td>3.354885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.182206</td>\n",
       "      <td>6.237578</td>\n",
       "      <td>4.194653</td>\n",
       "      <td>3.380361</td>\n",
       "      <td>5.511587</td>\n",
       "      <td>5.383889</td>\n",
       "      <td>8.941296</td>\n",
       "      <td>3.331588</td>\n",
       "      <td>8.257033</td>\n",
       "      <td>8.700136</td>\n",
       "      <td>...</td>\n",
       "      <td>4.016990</td>\n",
       "      <td>2.956002</td>\n",
       "      <td>2.622684</td>\n",
       "      <td>3.263552</td>\n",
       "      <td>3.606437</td>\n",
       "      <td>3.035578</td>\n",
       "      <td>2.938062</td>\n",
       "      <td>3.156967</td>\n",
       "      <td>3.055146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X100_g_at  X1000_at  X1001_at  X1002_f_at  X1003_s_at  X1004_at  X1005_at  \\\n",
       "0   7.234793  6.494211  4.853264    3.527822    5.575283  5.630715  7.070994   \n",
       "1   6.967237  6.632175  4.320490    3.535030    5.505270  5.173343  7.826527   \n",
       "2   7.026961  6.510959  4.267634    3.387379    5.906008  5.321219  7.857653   \n",
       "3   7.123875  6.155900  4.114608    3.380995    5.891499  5.602339  8.285221   \n",
       "4   7.182206  6.237578  4.194653    3.380361    5.511587  5.383889  8.941296   \n",
       "\n",
       "   X1006_at  X1007_s_at  X1008_f_at  ...  AFFX.ThrX.5_at  AFFX.ThrX.M_at  \\\n",
       "0  3.586507    8.607721    8.376150  ...        4.124928        3.130851   \n",
       "1  3.470474    6.871599    8.732676  ...        4.089809        3.030838   \n",
       "2  3.292397    7.521978    8.636165  ...        3.693827        2.755653   \n",
       "3  3.636381    8.148127    8.472201  ...        4.345752        3.122182   \n",
       "4  3.331588    8.257033    8.700136  ...        4.016990        2.956002   \n",
       "\n",
       "   AFFX.TrpnX.3_at  AFFX.TrpnX.5_at  AFFX.TrpnX.M_at  AFFX.YEL002c.WBP1_at  \\\n",
       "0         2.983105         3.286748         3.632831              3.200749   \n",
       "1         2.710369         3.204168         3.721313              3.080551   \n",
       "2         2.526112         3.254250         3.362329              2.862432   \n",
       "3         2.656120         3.530544         3.515947              3.026449   \n",
       "4         2.622684         3.263552         3.606437              3.035578   \n",
       "\n",
       "   AFFX.YEL018w._at  AFFX.YEL021w.URA3_at  AFFX.YEL024w.RIP1_at  Y  \n",
       "0          3.157482              3.572550              3.201209  1  \n",
       "1          2.908750              2.980353              3.264706  1  \n",
       "2          3.048200              3.247433              3.061890  1  \n",
       "3          3.231532              3.762868              3.354885  1  \n",
       "4          2.938062              3.156967              3.055146  1  \n",
       "\n",
       "[5 rows x 12626 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "NROWS = sys.maxsize\n",
    "#NROWS = 10\n",
    "\n",
    "df_chandran = pd.read_csv('./data/chandran.csv', sep=',', \n",
    "                     header=0, nrows = NROWS)\n",
    "display(df_chandran.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnMatch 0 100@g@at\n",
      "UnMatch 651 160020@at\n",
      "UnMatch 652 160021@r@at\n",
      "UnMatch 653 160022@at\n",
      "UnMatch 654 160023@at\n",
      "UnMatch 655 160024@at\n",
      "UnMatch 656 160025@at\n",
      "UnMatch 657 160026@at\n",
      "UnMatch 658 160027@s@at\n",
      "UnMatch 659 160028@s@at\n",
      "UnMatch 660 160029@at\n",
      "UnMatch 661 160030@at\n",
      "UnMatch 662 160031@at\n",
      "UnMatch 663 160032@at\n",
      "UnMatch 664 160033@s@at\n",
      "UnMatch 665 160034@s@at\n",
      "UnMatch 666 160035@at\n",
      "UnMatch 667 160036@at\n",
      "UnMatch 668 160037@at\n",
      "UnMatch 669 160038@s@at\n",
      "UnMatch 670 160039@at\n",
      "UnMatch 671 160040@at\n",
      "UnMatch 672 160041@at\n",
      "UnMatch 673 160042@s@at\n",
      "UnMatch 674 160043@at\n",
      "UnMatch 675 160044@g@at\n",
      "UnMatch 12610 AFFX@MurIL2@at\n",
      "(12599,)\n",
      "[    1     2     3 ... 12623 12624 12625]\n",
      "[    1     2     3 ... 12623 12624 12625]\n",
      "[11737 11738 11739 ...    66    65 12626]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1000_at</th>\n",
       "      <th>X1001_at</th>\n",
       "      <th>X1002_f_at</th>\n",
       "      <th>X1003_s_at</th>\n",
       "      <th>X1004_at</th>\n",
       "      <th>X1005_at</th>\n",
       "      <th>X1006_at</th>\n",
       "      <th>X1007_s_at</th>\n",
       "      <th>X1008_f_at</th>\n",
       "      <th>X1009_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX.ThrX.5_at</th>\n",
       "      <th>AFFX.ThrX.M_at</th>\n",
       "      <th>AFFX.TrpnX.3_at</th>\n",
       "      <th>AFFX.TrpnX.5_at</th>\n",
       "      <th>AFFX.TrpnX.M_at</th>\n",
       "      <th>AFFX.YEL002c.WBP1_at</th>\n",
       "      <th>AFFX.YEL018w._at</th>\n",
       "      <th>AFFX.YEL021w.URA3_at</th>\n",
       "      <th>AFFX.YEL024w.RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.494211</td>\n",
       "      <td>4.853264</td>\n",
       "      <td>3.527822</td>\n",
       "      <td>5.575283</td>\n",
       "      <td>5.630715</td>\n",
       "      <td>7.070994</td>\n",
       "      <td>3.586507</td>\n",
       "      <td>8.607721</td>\n",
       "      <td>8.376150</td>\n",
       "      <td>8.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>4.124928</td>\n",
       "      <td>3.130851</td>\n",
       "      <td>2.983105</td>\n",
       "      <td>3.286748</td>\n",
       "      <td>3.632831</td>\n",
       "      <td>3.200749</td>\n",
       "      <td>3.157482</td>\n",
       "      <td>3.572550</td>\n",
       "      <td>3.201209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.632175</td>\n",
       "      <td>4.320490</td>\n",
       "      <td>3.535030</td>\n",
       "      <td>5.505270</td>\n",
       "      <td>5.173343</td>\n",
       "      <td>7.826527</td>\n",
       "      <td>3.470474</td>\n",
       "      <td>6.871599</td>\n",
       "      <td>8.732676</td>\n",
       "      <td>7.820364</td>\n",
       "      <td>...</td>\n",
       "      <td>4.089809</td>\n",
       "      <td>3.030838</td>\n",
       "      <td>2.710369</td>\n",
       "      <td>3.204168</td>\n",
       "      <td>3.721313</td>\n",
       "      <td>3.080551</td>\n",
       "      <td>2.908750</td>\n",
       "      <td>2.980353</td>\n",
       "      <td>3.264706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.510959</td>\n",
       "      <td>4.267634</td>\n",
       "      <td>3.387379</td>\n",
       "      <td>5.906008</td>\n",
       "      <td>5.321219</td>\n",
       "      <td>7.857653</td>\n",
       "      <td>3.292397</td>\n",
       "      <td>7.521978</td>\n",
       "      <td>8.636165</td>\n",
       "      <td>7.880739</td>\n",
       "      <td>...</td>\n",
       "      <td>3.693827</td>\n",
       "      <td>2.755653</td>\n",
       "      <td>2.526112</td>\n",
       "      <td>3.254250</td>\n",
       "      <td>3.362329</td>\n",
       "      <td>2.862432</td>\n",
       "      <td>3.048200</td>\n",
       "      <td>3.247433</td>\n",
       "      <td>3.061890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.155900</td>\n",
       "      <td>4.114608</td>\n",
       "      <td>3.380995</td>\n",
       "      <td>5.891499</td>\n",
       "      <td>5.602339</td>\n",
       "      <td>8.285221</td>\n",
       "      <td>3.636381</td>\n",
       "      <td>8.148127</td>\n",
       "      <td>8.472201</td>\n",
       "      <td>7.894054</td>\n",
       "      <td>...</td>\n",
       "      <td>4.345752</td>\n",
       "      <td>3.122182</td>\n",
       "      <td>2.656120</td>\n",
       "      <td>3.530544</td>\n",
       "      <td>3.515947</td>\n",
       "      <td>3.026449</td>\n",
       "      <td>3.231532</td>\n",
       "      <td>3.762868</td>\n",
       "      <td>3.354885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.237578</td>\n",
       "      <td>4.194653</td>\n",
       "      <td>3.380361</td>\n",
       "      <td>5.511587</td>\n",
       "      <td>5.383889</td>\n",
       "      <td>8.941296</td>\n",
       "      <td>3.331588</td>\n",
       "      <td>8.257033</td>\n",
       "      <td>8.700136</td>\n",
       "      <td>8.103976</td>\n",
       "      <td>...</td>\n",
       "      <td>4.016990</td>\n",
       "      <td>2.956002</td>\n",
       "      <td>2.622684</td>\n",
       "      <td>3.263552</td>\n",
       "      <td>3.606437</td>\n",
       "      <td>3.035578</td>\n",
       "      <td>2.938062</td>\n",
       "      <td>3.156967</td>\n",
       "      <td>3.055146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1000_at  X1001_at  X1002_f_at  X1003_s_at  X1004_at  X1005_at  X1006_at  \\\n",
       "0  6.494211  4.853264    3.527822    5.575283  5.630715  7.070994  3.586507   \n",
       "1  6.632175  4.320490    3.535030    5.505270  5.173343  7.826527  3.470474   \n",
       "2  6.510959  4.267634    3.387379    5.906008  5.321219  7.857653  3.292397   \n",
       "3  6.155900  4.114608    3.380995    5.891499  5.602339  8.285221  3.636381   \n",
       "4  6.237578  4.194653    3.380361    5.511587  5.383889  8.941296  3.331588   \n",
       "\n",
       "   X1007_s_at  X1008_f_at  X1009_at  ...  AFFX.ThrX.5_at  AFFX.ThrX.M_at  \\\n",
       "0    8.607721    8.376150  8.000115  ...        4.124928        3.130851   \n",
       "1    6.871599    8.732676  7.820364  ...        4.089809        3.030838   \n",
       "2    7.521978    8.636165  7.880739  ...        3.693827        2.755653   \n",
       "3    8.148127    8.472201  7.894054  ...        4.345752        3.122182   \n",
       "4    8.257033    8.700136  8.103976  ...        4.016990        2.956002   \n",
       "\n",
       "   AFFX.TrpnX.3_at  AFFX.TrpnX.5_at  AFFX.TrpnX.M_at  AFFX.YEL002c.WBP1_at  \\\n",
       "0         2.983105         3.286748         3.632831              3.200749   \n",
       "1         2.710369         3.204168         3.721313              3.080551   \n",
       "2         2.526112         3.254250         3.362329              2.862432   \n",
       "3         2.656120         3.530544         3.515947              3.026449   \n",
       "4         2.622684         3.263552         3.606437              3.035578   \n",
       "\n",
       "   AFFX.YEL018w._at  AFFX.YEL021w.URA3_at  AFFX.YEL024w.RIP1_at  Y  \n",
       "0          3.157482              3.572550              3.201209  1  \n",
       "1          2.908750              2.980353              3.264706  1  \n",
       "2          3.048200              3.247433              3.061890  1  \n",
       "3          3.231532              3.762868              3.354885  1  \n",
       "4          2.938062              3.156967              3.055146  1  \n",
       "\n",
       "[5 rows x 12599 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000_at</th>\n",
       "      <th>1001_at</th>\n",
       "      <th>1002_f_at</th>\n",
       "      <th>1003_s_at</th>\n",
       "      <th>1004_at</th>\n",
       "      <th>1005_at</th>\n",
       "      <th>1006_at</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1008_f_at</th>\n",
       "      <th>1009_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "      <th>AFFX-YEL002c/WBP1_at</th>\n",
       "      <th>AFFX-YEL018w/_at</th>\n",
       "      <th>AFFX-YEL021w/URA3_at</th>\n",
       "      <th>AFFX-YEL024w/RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.391657</td>\n",
       "      <td>3.812922</td>\n",
       "      <td>3.453385</td>\n",
       "      <td>6.070151</td>\n",
       "      <td>5.527153</td>\n",
       "      <td>5.812353</td>\n",
       "      <td>3.167275</td>\n",
       "      <td>7.354981</td>\n",
       "      <td>9.419909</td>\n",
       "      <td>7.697655</td>\n",
       "      <td>...</td>\n",
       "      <td>3.770583</td>\n",
       "      <td>2.884436</td>\n",
       "      <td>2.730025</td>\n",
       "      <td>3.126168</td>\n",
       "      <td>2.870161</td>\n",
       "      <td>3.082210</td>\n",
       "      <td>2.747289</td>\n",
       "      <td>3.226588</td>\n",
       "      <td>3.480196</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.329050</td>\n",
       "      <td>3.958028</td>\n",
       "      <td>3.407226</td>\n",
       "      <td>5.921265</td>\n",
       "      <td>5.376464</td>\n",
       "      <td>7.303408</td>\n",
       "      <td>3.108708</td>\n",
       "      <td>7.391872</td>\n",
       "      <td>10.539579</td>\n",
       "      <td>8.544981</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190759</td>\n",
       "      <td>2.460119</td>\n",
       "      <td>2.696578</td>\n",
       "      <td>2.675271</td>\n",
       "      <td>2.940032</td>\n",
       "      <td>3.126269</td>\n",
       "      <td>3.013745</td>\n",
       "      <td>3.517859</td>\n",
       "      <td>3.428752</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.664007</td>\n",
       "      <td>3.783702</td>\n",
       "      <td>3.152019</td>\n",
       "      <td>5.452293</td>\n",
       "      <td>5.111794</td>\n",
       "      <td>7.207638</td>\n",
       "      <td>3.077360</td>\n",
       "      <td>7.488371</td>\n",
       "      <td>6.833428</td>\n",
       "      <td>8.448252</td>\n",
       "      <td>...</td>\n",
       "      <td>3.325183</td>\n",
       "      <td>2.603014</td>\n",
       "      <td>2.469759</td>\n",
       "      <td>2.615746</td>\n",
       "      <td>2.510172</td>\n",
       "      <td>2.730814</td>\n",
       "      <td>2.613696</td>\n",
       "      <td>2.823436</td>\n",
       "      <td>3.049716</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.469634</td>\n",
       "      <td>4.004581</td>\n",
       "      <td>3.341170</td>\n",
       "      <td>6.070925</td>\n",
       "      <td>5.296108</td>\n",
       "      <td>8.744059</td>\n",
       "      <td>3.117104</td>\n",
       "      <td>7.203028</td>\n",
       "      <td>10.400557</td>\n",
       "      <td>7.185107</td>\n",
       "      <td>...</td>\n",
       "      <td>3.625057</td>\n",
       "      <td>2.765521</td>\n",
       "      <td>2.681757</td>\n",
       "      <td>3.310741</td>\n",
       "      <td>3.197177</td>\n",
       "      <td>3.414182</td>\n",
       "      <td>3.193867</td>\n",
       "      <td>3.353537</td>\n",
       "      <td>3.567482</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.322408</td>\n",
       "      <td>4.242724</td>\n",
       "      <td>3.489324</td>\n",
       "      <td>6.141657</td>\n",
       "      <td>5.628390</td>\n",
       "      <td>6.825370</td>\n",
       "      <td>3.794904</td>\n",
       "      <td>7.403024</td>\n",
       "      <td>10.240322</td>\n",
       "      <td>7.163157</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698067</td>\n",
       "      <td>3.026876</td>\n",
       "      <td>2.691670</td>\n",
       "      <td>3.236030</td>\n",
       "      <td>3.003906</td>\n",
       "      <td>3.081497</td>\n",
       "      <td>2.963307</td>\n",
       "      <td>3.472050</td>\n",
       "      <td>3.598103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1000_at   1001_at  1002_f_at  1003_s_at   1004_at   1005_at   1006_at  \\\n",
       "0  7.391657  3.812922   3.453385   6.070151  5.527153  5.812353  3.167275   \n",
       "1  7.329050  3.958028   3.407226   5.921265  5.376464  7.303408  3.108708   \n",
       "2  7.664007  3.783702   3.152019   5.452293  5.111794  7.207638  3.077360   \n",
       "3  7.469634  4.004581   3.341170   6.070925  5.296108  8.744059  3.117104   \n",
       "4  7.322408  4.242724   3.489324   6.141657  5.628390  6.825370  3.794904   \n",
       "\n",
       "   1007_s_at  1008_f_at   1009_at  ...  AFFX-ThrX-5_at  AFFX-ThrX-M_at  \\\n",
       "0   7.354981   9.419909  7.697655  ...        3.770583        2.884436   \n",
       "1   7.391872  10.539579  8.544981  ...        3.190759        2.460119   \n",
       "2   7.488371   6.833428  8.448252  ...        3.325183        2.603014   \n",
       "3   7.203028  10.400557  7.185107  ...        3.625057        2.765521   \n",
       "4   7.403024  10.240322  7.163157  ...        3.698067        3.026876   \n",
       "\n",
       "   AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  AFFX-YEL002c/WBP1_at  \\\n",
       "0         2.730025         3.126168         2.870161              3.082210   \n",
       "1         2.696578         2.675271         2.940032              3.126269   \n",
       "2         2.469759         2.615746         2.510172              2.730814   \n",
       "3         2.681757         3.310741         3.197177              3.414182   \n",
       "4         2.691670         3.236030         3.003906              3.081497   \n",
       "\n",
       "   AFFX-YEL018w/_at  AFFX-YEL021w/URA3_at  AFFX-YEL024w/RIP1_at  Y  \n",
       "0          2.747289              3.226588              3.480196 -1  \n",
       "1          3.013745              3.517859              3.428752  1  \n",
       "2          2.613696              2.823436              3.049716 -1  \n",
       "3          3.193867              3.353537              3.567482 -1  \n",
       "4          2.963307              3.472050              3.598103  1  \n",
       "\n",
       "[5 rows x 12599 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1000_at</th>\n",
       "      <th>X1001_at</th>\n",
       "      <th>X1002_f_at</th>\n",
       "      <th>X1003_s_at</th>\n",
       "      <th>X1004_at</th>\n",
       "      <th>X1005_at</th>\n",
       "      <th>X1006_at</th>\n",
       "      <th>X1007_s_at</th>\n",
       "      <th>X1008_f_at</th>\n",
       "      <th>X1009_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX.ThrX.5_at</th>\n",
       "      <th>AFFX.ThrX.M_at</th>\n",
       "      <th>AFFX.TrpnX.3_at</th>\n",
       "      <th>AFFX.TrpnX.5_at</th>\n",
       "      <th>AFFX.TrpnX.M_at</th>\n",
       "      <th>AFFX.YEL002c.WBP1_at</th>\n",
       "      <th>AFFX.YEL018w._at</th>\n",
       "      <th>AFFX.YEL021w.URA3_at</th>\n",
       "      <th>AFFX.YEL024w.RIP1_at</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269</td>\n",
       "      <td>46</td>\n",
       "      <td>68</td>\n",
       "      <td>-11</td>\n",
       "      <td>-67</td>\n",
       "      <td>2059</td>\n",
       "      <td>43</td>\n",
       "      <td>1425</td>\n",
       "      <td>1073</td>\n",
       "      <td>2317</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>613</td>\n",
       "      <td>-2</td>\n",
       "      <td>41</td>\n",
       "      <td>-26</td>\n",
       "      <td>5</td>\n",
       "      <td>-15</td>\n",
       "      <td>75</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>-15</td>\n",
       "      <td>44</td>\n",
       "      <td>1885</td>\n",
       "      <td>25</td>\n",
       "      <td>1313</td>\n",
       "      <td>1521</td>\n",
       "      <td>2029</td>\n",
       "      <td>...</td>\n",
       "      <td>908</td>\n",
       "      <td>700</td>\n",
       "      <td>-8</td>\n",
       "      <td>13</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>-9</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310</td>\n",
       "      <td>24</td>\n",
       "      <td>93</td>\n",
       "      <td>-30</td>\n",
       "      <td>-8</td>\n",
       "      <td>652</td>\n",
       "      <td>-52</td>\n",
       "      <td>1648</td>\n",
       "      <td>1561</td>\n",
       "      <td>1287</td>\n",
       "      <td>...</td>\n",
       "      <td>1450</td>\n",
       "      <td>1102</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>-7</td>\n",
       "      <td>34</td>\n",
       "      <td>-4</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>2536</td>\n",
       "      <td>-35</td>\n",
       "      <td>1006</td>\n",
       "      <td>1307</td>\n",
       "      <td>2921</td>\n",
       "      <td>...</td>\n",
       "      <td>853</td>\n",
       "      <td>604</td>\n",
       "      <td>-4</td>\n",
       "      <td>8</td>\n",
       "      <td>-35</td>\n",
       "      <td>-4</td>\n",
       "      <td>-15</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>3845</td>\n",
       "      <td>-18</td>\n",
       "      <td>1009</td>\n",
       "      <td>1156</td>\n",
       "      <td>1924</td>\n",
       "      <td>...</td>\n",
       "      <td>931</td>\n",
       "      <td>664</td>\n",
       "      <td>-3</td>\n",
       "      <td>21</td>\n",
       "      <td>-16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1000_at  X1001_at  X1002_f_at  X1003_s_at  X1004_at  X1005_at  X1006_at  \\\n",
       "0       269        46          68         -11       -67      2059        43   \n",
       "1       245        45          18         -15        44      1885        25   \n",
       "2       310        24          93         -30        -8       652       -52   \n",
       "3       328        13          41          16        38      2536       -35   \n",
       "4       359        40          13          -1        42      3845       -18   \n",
       "\n",
       "   X1007_s_at  X1008_f_at  X1009_at  ...  AFFX.ThrX.5_at  AFFX.ThrX.M_at  \\\n",
       "0        1425        1073      2317  ...             873             613   \n",
       "1        1313        1521      2029  ...             908             700   \n",
       "2        1648        1561      1287  ...            1450            1102   \n",
       "3        1006        1307      2921  ...             853             604   \n",
       "4        1009        1156      1924  ...             931             664   \n",
       "\n",
       "   AFFX.TrpnX.3_at  AFFX.TrpnX.5_at  AFFX.TrpnX.M_at  AFFX.YEL002c.WBP1_at  \\\n",
       "0               -2               41              -26                     5   \n",
       "1               -8               13              -26                    14   \n",
       "2                9               27               -7                    34   \n",
       "3               -4                8              -35                    -4   \n",
       "4               -3               21              -16                     5   \n",
       "\n",
       "   AFFX.YEL018w._at  AFFX.YEL021w.URA3_at  AFFX.YEL024w.RIP1_at  Y  \n",
       "0               -15                    75                    16  1  \n",
       "1                -9                   137                     5  1  \n",
       "2                -4                    67                    16  1  \n",
       "3               -15                    70                    18  1  \n",
       "4                 1                    43                    12  1  \n",
       "\n",
       "[5 rows x 12599 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "NROWS = sys.maxsize\n",
    "#NROWS = 10\n",
    "SELECT_SAMPLES = 4\n",
    "SELECT_FEATURES_INI = 100\n",
    "SELECT_FEATURES_FIN = 110\n",
    "TEST_SIZE = 0.10\n",
    "PATH_DATA = './data'\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "def load_df(file):\n",
    "    \"\"\"\n",
    "    Load sample files\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('./data' + '/' + file, sep=',', header=0, nrows = NROWS)\n",
    "    return df\n",
    "\n",
    "def normalize_feature_names(features):\n",
    "    \"\"\"\n",
    "    Normalize the names of the features in order to select the common features\n",
    "    \"\"\"\n",
    "    features_new = []\n",
    "    for idx, feature in enumerate(features):\n",
    "        feature_new = feature.replace('/', '@').replace('-', '@').replace('_', '@').replace('.', '@')\n",
    "        if feature[0] == \"X\":\n",
    "            features_new.append(feature_new[1:])\n",
    "        else:\n",
    "            features_new.append(feature_new)\n",
    "    return features_new\n",
    "\n",
    "def intersect_features(df_samples):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    features_0 = df_samples[0].columns\n",
    "    features_1 = df_samples[1].columns\n",
    "    features_2 = df_samples[2].columns\n",
    "    norm_features_0 = np.array(normalize_feature_names(features_0))\n",
    "    norm_features_1 = np.array(normalize_feature_names(features_1))\n",
    "    norm_features_2 = np.array(normalize_feature_names(features_2))\n",
    "    intersect_0 = np.array([], dtype=int)\n",
    "    intersect_1 = np.array([], dtype=int)\n",
    "    intersect_2 = np.array([], dtype=int)\n",
    "    for idx_0, feature_0 in enumerate(norm_features_0):\n",
    "        idx_1 = np.where(norm_features_1 == feature_0)\n",
    "        idx_2 = np.where(norm_features_2 == feature_0)\n",
    "        if idx_1[0] and idx_2[0]:\n",
    "            intersect_0 = np.append(intersect_0, idx_0)\n",
    "            intersect_1 = np.append(intersect_1, idx_1[0])\n",
    "            intersect_2 = np.append(intersect_2, idx_2[0])\n",
    "        else:\n",
    "            print(\"UnMatch\", idx_0, feature_0)\n",
    "    print(intersect_0.shape)\n",
    "    print(intersect_0)\n",
    "    print(intersect_1)\n",
    "    print(intersect_2)\n",
    "    df_samples_norm = []\n",
    "    df_samples_norm.append(df_samples[0].iloc[:, intersect_0])\n",
    "    df_samples_norm.append(df_samples[1].iloc[:, intersect_1])\n",
    "    df_samples_norm.append(df_samples[2].iloc[:, intersect_2])\n",
    "    return df_samples_norm\n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Split data\n",
    "    \"\"\"\n",
    "    labels = np.concatenate([df_chandran.iloc[:SELECT_SAMPLES, -1], \n",
    "                             df_chandran.iloc[-SELECT_SAMPLES:, -1]], axis = 0)\n",
    "\n",
    "    features = np.concatenate([df_chandran.iloc[:SELECT_SAMPLES, SELECT_FEATURES_INI:SELECT_FEATURES_FIN], \n",
    "                               df_chandran.iloc[-SELECT_SAMPLES:, SELECT_FEATURES_INI:SELECT_FEATURES_FIN]], axis = 0)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "df_samples = []\n",
    "for file in ['chandran.csv','singh.csv', 'welsh.csv']:\n",
    "    df_samples.append(load_df(file))\n",
    "\n",
    "df_samples_norm = intersect_features(df_samples)\n",
    "\n",
    "# Standardize the 0 label as -1 in dataset 1\n",
    "mask = df_samples_norm[1][\"Y\"] == 0\n",
    "df_samples_norm[1].loc[mask, \"Y\"] = -1\n",
    "\n",
    "for i in range(3):\n",
    "    display(df_samples_norm[i].head()) \n",
    "    \n",
    "#     X_train = []\n",
    "#     X_test = []\n",
    "#     y_train = [] \n",
    "#     y_test = [] \n",
    "#     idx = 0\n",
    "#     X_tr, X_t, y_tr, y_t = split_data(df_samples[idx])\n",
    "#     X_train.append(X_tr)\n",
    "#     X_test.append(X_t)\n",
    "#     y_train.append(y_tr)\n",
    "#     y_test.append(y_t)\n",
    "#     features = df_samples[idx].columns[SELECT_FEATURES_INI:SELECT_FEATURES_FIN].values\n",
    "#     print(features)\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (5, 10)\n",
      "First final_X [[3.04267208 7.19191304 7.82334842 2.51988026 4.43116838 2.28238102\n",
      "  6.20866444 3.97344996 2.58739816 3.38964881]\n",
      " [2.95738953 7.14597925 8.02878747 3.09612524 4.88298697 2.4350044\n",
      "  5.69849878 4.77639802 2.7831269  5.01608014]\n",
      " [2.96428904 7.18547986 7.83712188 3.21338741 4.88506596 2.41277747\n",
      "  5.65483307 4.73698059 2.75456921 3.76447161]\n",
      " [3.43724992 7.60105699 7.93612435 2.9981392  5.01502853 2.5739303\n",
      "  5.82747709 4.93601557 2.86524365 4.73670692]\n",
      " [3.19159297 7.44015753 8.01133704 2.9925223  5.12199766 2.2825365\n",
      "  5.78955862 5.44155007 2.69530392 3.59354046]]\n",
      "Z [ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]\n",
      "#Features 10\n",
      "#Elimination order [0 0 0 0 0 0 0 0 0 0]\n",
      "#Init feature indices [0 1 2 3 4 5 6 7 8 9]\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=0,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "ones [1. 1. 1. 1. 1.]\n",
      "outer [[ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1. 10.  1.  1.  1.  1.]]\n",
      "final_X [[ 3.04267208  7.19191304  7.82334842  2.51988026  4.43116838 22.82381017\n",
      "   6.20866444  3.97344996  2.58739816  3.38964881]\n",
      " [ 2.95738953  7.14597925  8.02878747  3.09612524  4.88298697 24.35004395\n",
      "   5.69849878  4.77639802  2.7831269   5.01608014]\n",
      " [ 2.96428904  7.18547986  7.83712188  3.21338741  4.88506596 24.1277747\n",
      "   5.65483307  4.73698059  2.75456921  3.76447161]\n",
      " [ 3.43724992  7.60105699  7.93612435  2.9981392   5.01502853 25.73930302\n",
      "   5.82747709  4.93601557  2.86524365  4.73670692]\n",
      " [ 3.19159297  7.44015753  8.01133704  2.9925223   5.12199766 22.82536502\n",
      "   5.78955862  5.44155007  2.69530392  3.59354046]]\n",
      "Coefs [[-0.63883666 -0.59585623  0.00470063  0.53570808 -0.05200796 -0.086136\n",
      "  -0.45599739 -0.19059545  0.0266577   0.54385364]]\n",
      "ones [1. 1. 1. 1. 1.]\n",
      "outer [[6.38836659e-01 5.95856230e-01 4.70062756e-03 5.35708085e-01\n",
      "  5.20079591e-02 8.61359969e+00 4.55997389e-01 1.90595448e-01\n",
      "  2.66577036e-02 5.43853644e-01]\n",
      " [6.38836659e-01 5.95856230e-01 4.70062756e-03 5.35708085e-01\n",
      "  5.20079591e-02 8.61359969e+00 4.55997389e-01 1.90595448e-01\n",
      "  2.66577036e-02 5.43853644e-01]\n",
      " [6.38836659e-01 5.95856230e-01 4.70062756e-03 5.35708085e-01\n",
      "  5.20079591e-02 8.61359969e+00 4.55997389e-01 1.90595448e-01\n",
      "  2.66577036e-02 5.43853644e-01]\n",
      " [6.38836659e-01 5.95856230e-01 4.70062756e-03 5.35708085e-01\n",
      "  5.20079591e-02 8.61359969e+00 4.55997389e-01 1.90595448e-01\n",
      "  2.66577036e-02 5.43853644e-01]\n",
      " [6.38836659e-01 5.95856230e-01 4.70062756e-03 5.35708085e-01\n",
      "  5.20079591e-02 8.61359969e+00 4.55997389e-01 1.90595448e-01\n",
      "  2.66577036e-02 5.43853644e-01]]\n",
      "final_X [[ 1.94377047  4.2853462   0.03677465  1.34992023  0.23045602 19.65951643\n",
      "   2.83113477  0.75732147  0.06897409  1.84347286]\n",
      " [ 1.88928885  4.25797626  0.03774034  1.65861932  0.25395419 20.97415311\n",
      "   2.59850057  0.91035972  0.07419177  2.72801346]\n",
      " [ 1.8936965   4.28151294  0.03683939  1.72143761  0.25406231 20.78269928\n",
      "   2.57858911  0.90284694  0.07343049  2.0473216 ]\n",
      " [ 2.19584126  4.52913717  0.03730476  1.60612741  0.2608214  22.17080527\n",
      "   2.65731434  0.9407821   0.07638082  2.57607532]\n",
      " [ 2.03890659  4.43326422  0.03765831  1.60311839  0.26638464 19.66085571\n",
      "   2.64002362  1.03713467  0.07185061  1.95436008]]\n",
      "Coefs [[-4.24467227e-01 -3.82980417e-01 -1.60469902e-04  2.36437632e-01\n",
      "  -9.97857931e-03 -3.19819798e-02 -1.69451346e-01 -9.38722623e-02\n",
      "   2.12491433e-04  2.84140501e-01]]\n",
      "ones [1. 1. 1. 1. 1.]\n",
      "outer [[2.71165225e-01 2.28201267e-01 7.54309246e-07 1.26661551e-01\n",
      "  5.18965545e-04 2.75479972e+00 7.72693715e-02 1.78916258e-02\n",
      "  5.66453364e-06 1.54530847e-01]\n",
      " [2.71165225e-01 2.28201267e-01 7.54309246e-07 1.26661551e-01\n",
      "  5.18965545e-04 2.75479972e+00 7.72693715e-02 1.78916258e-02\n",
      "  5.66453364e-06 1.54530847e-01]\n",
      " [2.71165225e-01 2.28201267e-01 7.54309246e-07 1.26661551e-01\n",
      "  5.18965545e-04 2.75479972e+00 7.72693715e-02 1.78916258e-02\n",
      "  5.66453364e-06 1.54530847e-01]\n",
      " [2.71165225e-01 2.28201267e-01 7.54309246e-07 1.26661551e-01\n",
      "  5.18965545e-04 2.75479972e+00 7.72693715e-02 1.78916258e-02\n",
      "  5.66453364e-06 1.54530847e-01]\n",
      " [2.71165225e-01 2.28201267e-01 7.54309246e-07 1.26661551e-01\n",
      "  5.18965545e-04 2.75479972e+00 7.72693715e-02 1.78916258e-02\n",
      "  5.66453364e-06 1.54530847e-01]]\n",
      "final_X [[8.25066861e-01 1.64120367e+00 5.90122404e-06 3.19171942e-01\n",
      "  2.29962371e-03 6.28750258e+00 4.79739598e-01 7.10914800e-02\n",
      "  1.46564039e-05 5.23805301e-01]\n",
      " [8.01941198e-01 1.63072152e+00 6.05618862e-06 3.92160024e-01\n",
      "  2.53410199e-03 6.70794942e+00 4.40319419e-01 8.54575263e-02\n",
      "  1.57651160e-05 7.75139113e-01]\n",
      " [8.03812105e-01 1.63973561e+00 5.91161349e-06 4.07012633e-01\n",
      "  2.53518092e-03 6.64671869e+00 4.36945397e-01 8.47522843e-02\n",
      "  1.56033500e-05 5.81726986e-01]\n",
      " [9.32062650e-01 1.73457084e+00 5.98629197e-06 3.79748960e-01\n",
      "  2.60262701e-03 7.09066247e+00 4.50285492e-01 8.83133437e-02\n",
      "  1.62302691e-05 7.31967332e-01]\n",
      " [8.65449027e-01 1.69785338e+00 6.04302560e-06 3.79037515e-01\n",
      "  2.65814030e-03 6.28793091e+00 4.47355556e-01 9.73581778e-02\n",
      "  1.52676398e-05 5.55312851e-01]]\n",
      "Coefs [[-1.49896950e-01 -1.04502010e-01  7.90262400e-08  1.00267553e-01\n",
      "   1.65799427e-04 -5.67105033e-03 -5.26952106e-02  1.06041325e-02\n",
      "   5.03169600e-07  1.05016350e-01]]\n",
      "@@Z to remove [4.06468402e-02 2.38474912e-02 5.96102235e-14 1.27000437e-02\n",
      " 8.60441902e-08 1.56226079e-01 4.07172581e-03 1.89725171e-04\n",
      " 2.85022113e-12 1.62282655e-02] [5.96102235e-14 2.85022113e-12]\n",
      "@@@Threshold [False False  True False False False False False  True False]\n",
      "@@@Remove order [0 1]\n",
      "@@@Elimination order [2 8]\n",
      "@@@Elimination order 2 [2 8]\n",
      "@@@Elimination order [2 8]\n",
      "@@@Elimination order 2 [2 8]\n",
      "@@@@elimination_order 3 [0 0 1 0 0 0 0 0 2 0]\n",
      "ones [1. 1. 1. 1. 1.]\n",
      "outer [[4.06468402e-02 2.38474912e-02 1.27000437e-02 8.60441902e-08\n",
      "  1.56226079e-01 4.07172581e-03 1.89725171e-04 1.62282655e-02]\n",
      " [4.06468402e-02 2.38474912e-02 1.27000437e-02 8.60441902e-08\n",
      "  1.56226079e-01 4.07172581e-03 1.89725171e-04 1.62282655e-02]\n",
      " [4.06468402e-02 2.38474912e-02 1.27000437e-02 8.60441902e-08\n",
      "  1.56226079e-01 4.07172581e-03 1.89725171e-04 1.62282655e-02]\n",
      " [4.06468402e-02 2.38474912e-02 1.27000437e-02 8.60441902e-08\n",
      "  1.56226079e-01 4.07172581e-03 1.89725171e-04 1.62282655e-02]\n",
      " [4.06468402e-02 2.38474912e-02 1.27000437e-02 8.60441902e-08\n",
      "  1.56226079e-01 4.07172581e-03 1.89725171e-04 1.62282655e-02]]\n",
      "final_X [[1.23675006e-01 1.71509083e-01 3.20025895e-02 3.81276295e-07\n",
      "  3.56567436e-01 2.52799792e-02 7.53863471e-04 5.50081208e-02]\n",
      " [1.20208540e-01 1.70413677e-01 3.93209259e-02 4.20152660e-07\n",
      "  3.80411188e-01 2.32027245e-02 9.06202930e-04 8.14022803e-02]\n",
      " [1.20488983e-01 1.71355668e-01 4.08101606e-02 4.20331544e-07\n",
      "  3.76938762e-01 2.30249297e-02 8.98724450e-04 6.10908447e-02]\n",
      " [1.39713348e-01 1.81266140e-01 3.80764989e-02 4.31514068e-07\n",
      "  4.02115038e-01 2.37278888e-02 9.36486396e-04 7.68685375e-02]\n",
      " [1.29728170e-01 1.77429091e-01 3.80051641e-02 4.40718141e-07\n",
      "  3.56591727e-01 2.35734953e-02 1.03239902e-03 5.83169287e-02]]\n",
      "Coefs [[-2.87439954e-02 -1.69258858e-02  4.04942348e-03 -3.17480047e-08\n",
      "  -1.35681394e-03 -1.07372983e-03 -1.63958031e-04  7.30765885e-03]]\n",
      "@@Z to remove [1.16835259e-03 4.03639913e-04 5.14278552e-05 2.73173135e-15\n",
      " 2.11969721e-03 4.37193345e-06 3.11069655e-08 1.18590628e-04] [2.73173135e-15]\n",
      "@@@Threshold [False False False  True False False False False]\n",
      "@@@Remove order [0]\n",
      "@@@Elimination order [4]\n",
      "@@@Elimination order 2 [4]\n",
      "@@@Elimination order [4]\n",
      "@@@Elimination order 2 [4]\n",
      "@@@@elimination_order 3 [0 0 1 0 3 0 0 0 2 0]\n",
      "ones [1. 1. 1. 1. 1.]\n",
      "outer [[1.16835259e-03 4.03639913e-04 5.14278552e-05 2.11969721e-03\n",
      "  4.37193345e-06 3.11069655e-08 1.18590628e-04]\n",
      " [1.16835259e-03 4.03639913e-04 5.14278552e-05 2.11969721e-03\n",
      "  4.37193345e-06 3.11069655e-08 1.18590628e-04]\n",
      " [1.16835259e-03 4.03639913e-04 5.14278552e-05 2.11969721e-03\n",
      "  4.37193345e-06 3.11069655e-08 1.18590628e-04]\n",
      " [1.16835259e-03 4.03639913e-04 5.14278552e-05 2.11969721e-03\n",
      "  4.37193345e-06 3.11069655e-08 1.18590628e-04]\n",
      " [1.16835259e-03 4.03639913e-04 5.14278552e-05 2.11969721e-03\n",
      "  4.37193345e-06 3.11069655e-08 1.18590628e-04]]\n",
      "final_X [[3.55491380e-03 2.90294315e-03 1.29592037e-04 4.83795667e-03\n",
      "  2.71438677e-05 1.23601971e-07 4.01980581e-04]\n",
      " [3.45527371e-03 2.88440244e-03 1.59227080e-04 5.16147202e-03\n",
      "  2.49134574e-05 1.48579248e-07 5.94860094e-04]\n",
      " [3.46333477e-03 2.90034646e-03 1.65257622e-04 5.11435767e-03\n",
      "  2.47225538e-05 1.47353092e-07 4.46431052e-04]\n",
      " [4.01591984e-03 3.06808998e-03 1.54187869e-04 5.45595288e-03\n",
      "  2.54773420e-05 1.53544466e-07 5.61729048e-04]\n",
      " [3.72890591e-03 3.00314453e-03 1.53899004e-04 4.83828625e-03\n",
      "  2.53115650e-05 1.69270110e-07 4.26160220e-04]]\n",
      "Coefs [[-8.26217272e-04 -2.86485610e-04  1.63978305e-05 -1.84094406e-05\n",
      "  -1.15289574e-06 -2.68822361e-08  5.34018778e-05]]\n",
      "@@Z to remove [9.65313088e-07 1.15637027e-07 8.43305252e-10 3.90224399e-07\n",
      " 5.04038346e-12 8.36224789e-16 6.33296222e-09] [5.04038346e-12 8.36224789e-16]\n",
      "@@@Threshold [False False False False  True  True False]\n",
      "@@@Remove order [1 0]\n",
      "@@@Elimination order [6 7]\n",
      "@@@Elimination order 2 [7 6]\n",
      "@@@Elimination order [6 7]\n",
      "@@@Elimination order 2 [7 6]\n",
      "@@@@elimination_order 3 [0 0 1 0 3 0 5 4 2 0]\n",
      "[10  8  1  6  3  9  5  4  2  7]\n",
      "[2 8 4 7 6 3 9 1 5 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 5, 1, 9, 3, 6, 7, 4, 8, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# X is numpy array witht the data (rows are data instances)\n",
    "# Y is a numpy vector with the class labels (-1 or 1)\n",
    "# C is the regularization coefficient of the SVM\n",
    "# threshold is the threshold value to drop features in L2AROM\n",
    "\n",
    "print(\"Shape\", X_train.shape)\n",
    "# b = np.ones(X_train.shape[1])\n",
    "b = np.array([1, 1, 1, 1, 1, 10, 1, 1, 1, 1], dtype=float)\n",
    "variable_ranking(X_train, y_train, C = 1, b=b, threshold=1e-10, feature_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chandran.csv\n",
      "singh.csv\n",
      "welsh.csv\n",
      "     105 data/chandran.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data\n",
    "wc -l data/chandran.csv\n",
    "#head -n 1 data/chandran.csv\n",
    "#array([0, 9, 1, 3, 6, 7, 4, 8, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42147589 -0.68077729 -1.21921664 -1.88058457 -1.85117099 -1.05603962\n",
      "   1.89860388 -1.6935514  -1.61454494 -1.09149417]\n",
      " [-0.89463816 -0.93920353  1.18930025  0.55941249  0.06680582  0.34616531\n",
      "  -0.69917435  0.00745521  0.49600344  1.40729302]\n",
      " [-0.8563585  -0.71697077 -1.05773997  1.05593632  0.07563114  0.14195869\n",
      "  -0.92152144 -0.07604871  0.18806506 -0.51563069]\n",
      " [ 1.76770977  1.62109074  0.10294062  0.14450971  0.62732422  1.62252674\n",
      "  -0.04241309  0.34559726  1.3814706   0.97807463]\n",
      " [ 0.40476279  0.71586084  0.98471574  0.12072605  1.08140982 -1.05461113\n",
      "  -0.23549501  1.41654763 -0.45099415 -0.77824279]]\n",
      "(5, 10)\n",
      "t [-1.77057815 -1.5894688   0.09316163  1.51944707  0.10088885  0.35222012\n",
      " -1.52838138 -0.04852196  0.50375244  0.6769448 ] p [0.17476488 0.2101723  0.93164814 0.22596742 0.92600332 0.74796361\n",
      " 0.22388075 0.96434991 0.6491003  0.54696565]\n",
      "(5, 5)\n",
      "[103 109 108 105 104]\n",
      "['X1090_f_at' 'X1091_at' 'X1092_at' 'X1093_at' 'X1094_g_at' 'X1095_s_at'\n",
      " 'X1096_g_at' 'X1097_s_at' 'X1098_at' 'X1099_s_at']\n",
      "['X1093_at' 'X1099_s_at' 'X1098_at' 'X1095_s_at' 'X1094_g_at']\n"
     ]
    }
   ],
   "source": [
    "# We need to nake this with t-test\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, MaxAbsScaler\n",
    "from scipy import stats\n",
    "\n",
    "def ttest(X, y):\n",
    "    \"\"\"\n",
    "    Score statistic function for transformer GenericUnivariate\n",
    "    \"\"\"\n",
    "    t, p = stats.ttest_ind(X[y==1] , X[y==-1])\n",
    "    print('t',t, 'p', p)\n",
    "    return t,p\n",
    "\n",
    "#X, y = load_breast_cancer(return_X_y=True)\n",
    "def select_features(X, y, num_features=3):\n",
    "    \"\"\"\n",
    "    Select the features with the \n",
    "    \"\"\"\n",
    "    print(X.shape)\n",
    "    transformer = GenericUnivariateSelect(ttest, 'k_best', param=num_features)\n",
    "    X_new = transformer.fit_transform(X, y)\n",
    "    print(X_new.shape)\n",
    "    feature_indexes = SELECT_FEATURES_INI + np.argsort(transformer.scores_)[::-1][0:num_features]\n",
    "    return feature_indexes\n",
    "\n",
    "for i in range(3):\n",
    "    display(df_samples_norm[i].head()) \n",
    "\n",
    "# First version\n",
    "#     X_train = []\n",
    "#     X_test = []\n",
    "#     y_train = [] \n",
    "#     y_test = [] \n",
    "#     idx = 0\n",
    "#     X_tr, X_t, y_tr, y_t = split_data(df_samples[idx])\n",
    "#     X_train.append(X_tr)\n",
    "#     X_test.append(X_t)\n",
    "#     y_train.append(y_tr)\n",
    "#     y_test.append(y_t)\n",
    "#     features = df_samples[idx].columns[SELECT_FEATURES_INI:SELECT_FEATURES_FIN].values\n",
    "#     print(features)\n",
    "#     idx += 1\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "print(X_train_scaled)\n",
    "indexes = select_features(X_train_scaled, y_train, num_features=5)\n",
    "print(indexes)\n",
    "\n",
    "features = df_chandran.columns[SELECT_FEATURES_INI:SELECT_FEATURES_FIN].values\n",
    "print(features)\n",
    "best_features = df_chandran.columns[indexes].values\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 -1 -1]\n",
      "[array([ True, False, False,  True,  True])]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print( [ y_train==-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = [ 6.66235409 15.08939652 16.52736128  6.2786939  10.05689064  5.2693547\n",
      " 12.08027011  9.1448739   5.9687117   7.31128142]\n",
      "p = [1.58762463e-04 3.67991267e-07 1.81320448e-07 2.38199665e-04\n",
      " 8.13605052e-06 7.55884801e-04 2.03763261e-06 1.64760146e-05\n",
      " 3.34888705e-04 8.29781945e-05]\n",
      "t = [-1.77057815 -1.5894688   0.09316163  1.51944707  0.10088885  0.35222012\n",
      " -1.52838138 -0.04852196  0.50375244  0.6769448 ]\n",
      "p = [0.17476488 0.2101723  0.93164814 0.22596742 0.92600332 0.74796361\n",
      " 0.22388075 0.96434991 0.6491003  0.54696565]\n",
      "(5, 10)\n",
      "[[3.04267208 7.19191304 7.82334842 2.51988026 4.43116838 2.28238102\n",
      "  6.20866444 3.97344996 2.58739816 3.38964881]\n",
      " [2.95738953 7.14597925 8.02878747 3.09612524 4.88298697 2.4350044\n",
      "  5.69849878 4.77639802 2.7831269  5.01608014]\n",
      " [2.96428904 7.18547986 7.83712188 3.21338741 4.88506596 2.41277747\n",
      "  5.65483307 4.73698059 2.75456921 3.76447161]\n",
      " [3.43724992 7.60105699 7.93612435 2.9981392  5.01502853 2.5739303\n",
      "  5.82747709 4.93601557 2.86524365 4.73670692]\n",
      " [3.19159297 7.44015753 8.01133704 2.9925223  5.12199766 2.2825365\n",
      "  5.78955862 5.44155007 2.69530392 3.59354046]]\n",
      "[-1  1  1 -1 -1]\n",
      "[2.95738953 2.96428904]\n",
      "[3.04267208 3.43724992 3.19159297]\n",
      "t = -1.5894687976376054\n",
      "p = 0.21017230301088344\n"
     ]
    }
   ],
   "source": [
    "# T-test\n",
    "\n",
    "## Import the packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Define 2 random distributions\n",
    "#Sample Size\n",
    "N = 10\n",
    "np.random.seed(0)\n",
    "#Gaussian distributed data with mean = 2 and var = 1\n",
    "a = np.random.randn(N) + 2\n",
    "#Gaussian distributed data with with mean = 0 and var = 1\n",
    "b = np.random.randn(N)\n",
    "\n",
    "\n",
    "t2, p2 = stats.ttest_ind(X_train, y_train)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))\n",
    "\n",
    "t2, p2 = stats.ttest_ind(X_train[y_train==1] , X_train[y_train==-1])\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "a = X_train[y_train==1].T\n",
    "b = X_train[y_train==-1].T\n",
    "\n",
    "print(a[0])\n",
    "print(b[0])\n",
    "# print(y_train)\n",
    "# print(X_train[y_train==1])\n",
    "\n",
    "# print()\n",
    "# print(X_train[y_train==-1])\n",
    "\n",
    "t2, p2 = stats.ttest_ind(a[1] , b[1])\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_metadata": {
     "hidden": "true"
    }
   },
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "latex_metadata": {
     "hidden": "true",
     "lexer": "bash"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook feature_selection_linear_models.ipynb to latex\n",
      "[NbConvertApp] Writing 32105 bytes to feature_selection_linear_models.tex\n",
      "[NbConvertApp] Converting notebook feature_selection_linear_models.ipynb to html_with_toclenvs\n",
      "[NbConvertApp] Writing 285913 bytes to feature_selection_linear_models.html\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "jupyter nbconvert --to=latex --template=~/report.tplx feature_selection_linear_models.ipynb 1> /dev/null\n",
    "pdflatex -shell-escape feature_selection_linear_models 1> /dev/null\n",
    "jupyter nbconvert --to html_with_toclenvs feature_selection_linear_models.ipynb 1> /dev/null"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "latex_metadata": {
   "author": "Daniel Cerdán, Fernando Freire",
   "title": "Partially Supervised Feature Selection with Regularized Linear Models"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
